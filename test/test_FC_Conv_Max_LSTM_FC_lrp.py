from lrp import lrp
import tensorflow as tf
import numpy as np
import unittest


class FCConvMaxLSTMFCTest(unittest.TestCase):
    def runTest(self):
        # Build the computational graph
        with tf.Graph().as_default() as g:
            # Make static input shape: (8, 10)
            inp = tf.constant([[-2, -17, 17, 4, 8, -1, 0, -16, -16, 17],
                               [11, 18, -2, -10, -7, 5, -11, 18, 0, -10],
                               [2, -7, -17, -15, 3, 11, -5, -11, 18, 4],
                               [4, 5, 9, -19, 10, 8, 8, 13, 1, -7],
                               [9, 0, -6, -7, 19, 20, 4, 8, 2, -3],
                               [-2, -17, 0, 10, -17, 17, 19, -15, 2, -7],
                               [7, 14, 11, 16, -1, 10, -9, 3, -4, -7],
                               [11, 1, 1, 16, 0, 14, 2, 20, -20, 13]], dtype=tf.float32)

            # -------------------------------------------- FC 1--------------------------------------------
            # shape: (10, 6)
            weights_1 = tf.constant([[0.99215692, 0.27058825, 0.34509805, 0.99215692, -0.98823535, 0.322352958],
                                     [0.87843144, -0.17254902, 0.50196081, -0.98823535, 0.98823535, 0.241568646],
                                     [-0.54509807, 0.0627451, 0.64705884, -0.1647058, 0.94117653, 0.18823532],
                                     [0.20000002, -0.77647066, 0.99215692, 0.88627458, 0.77254909, 0.41490199],
                                     [0.98823535, 0.10196079, 0.84705889, 0.9450981, 0.22352943, -0.91372555],
                                     [0.9333334, 0.8705883, 0.10980393, -0.20000002, 0.22352943, 0.42745101],
                                     [0.14901961, 0.98823535, -0.0509804, 0.1137255, 0.56470591, 0.07450981],
                                     [0.77254909, 0.88235301, 0.40784317, 0.98823535, 0.80784321, 0.65882355],
                                     [0.98823535, -0.0627451, 0.99215692, -0.98823535, 0.99215692, 0.91764712],
                                     [0.88235301, 0.84705889, 0.99215692, -0.64705884, 0.98823535, 0.98823535]],
                                    dtype=tf.float32)

            # shape: (6,)
            bias_1 = tf.constant([-1.5, -1, -0.5, 0, 0.5, 1], dtype=tf.float32)

            # shape: (8, 6)
            output_1 = tf.matmul(inp, weights_1) + bias_1

            # -------------------------------------------- Convolution 1 --------------------------------------------
            # Create the filter which has shape [filter_width, in_channels, out_channels]
            filter_2 = tf.constant(
                [[[0.14901961, 0.99215692, -0.01176471, -0.85490203],
                  [0.98823535, 0.32156864, 0.49803925, 0.08235294],
                  [-0.0509804, -0.09803922, -0.99215692, 0.66274512],
                  [0.1137255, 0.99215692, 0.99215692, -0.99215692],
                  [0.56470591, -0.99215692, 0.66274512, 0.99215692],
                  [0.07450981, 0.84705889, -0.30588236, -0.52156866]],
                 [[0.14901961, 0.09019608, 0.99215692, 0.1254902],
                  [0.98823535, -0.19215688, -0.99215692, 0.50980395],
                  [-0.0509804, 0.99215692, 0.65882355, -0.99607849],
                  [0.1137255, -0.99215692, 0.27843139, 0.99215692],
                  [0.56470591, 0.76862752, -0.99215692, -0.99215692],
                  [0.07450981, 0.04705883, 0.99215692, 0.44313729]]],
                dtype=tf.float32)

            bias_2 = tf.constant([0.84705889, 0.09019608, -0.19215688, 0.99215692], dtype=tf.float32)

            # Add an extra dimension to fit the expected input shape of conv1d
            output_1_reshaped = tf.expand_dims(output_1, 0)
            output_2 = tf.nn.conv1d(output_1_reshaped, filter_2, 1, "SAME") + bias_2

            # -------------------------------------------- Max pooling --------------------------------------------

            # Pooling is defined for 2d, so add dim of 1 (height)
            output_2_reshaped = tf.expand_dims(output_2, 1)
            # Kernel looks at 1 sample, 1 height, 2 width, and 1 depth
            ksize = [1, 1, 2, 1]

            # Move 1 sample, 1 height, 2 width, and 1 depth at a time
            strides = [1, 1, 2, 1]

            # Perform the max pooling
            pool = tf.nn.max_pool(output_2_reshaped, ksize, strides, padding='SAME')

            # Remove the "height" dimension again
            output_3 = tf.squeeze(pool, 1)

            # -------------------------------------------- Convolution 2--------------------------------------------

            # Create the filter which has shape [filter_width, in_channels, out_channels]
            filter_4 = tf.constant(
                [[[-0.99215692, 0.99215692, -0.52156866, 0.1254902],
                  [0.50980395, -0.99607849, 0.99215692, -0.99215692],
                  [0.44313729, 0.50588238, -0.99215692, 0.99215692],
                  [0.33725491, 0.84313732, -0.99215692, 0.99607849]],
                 [[0.71764708, 0.0627451, -0.19215688, 0.99215692],
                  [-0.19215688, 0.99215692, -0.99215692, 0.99215692],
                  [0.99215692, -0.98039222, 0.56862748, 0.05490196],
                  [-0.60392159, 0.99215692, 0.65882355, -0.27450982]]],
                dtype=tf.float32)

            bias_4 = tf.constant([0.07450981, 0.14901961, 0.98823535, -0.0509804], dtype=tf.float32)

            output_4 = tf.nn.conv1d(output_3, filter_4, 1, "SAME") + bias_4

            # -------------------------------------------- Max pooling --------------------------------------------

            # Pooling is defined for 2d, so add dim of 1 (height)
            output_2_reshaped = tf.expand_dims(output_4, 1)
            # Kernel looks at 1 sample, 1 height, 2 width, and 1 depth
            ksize = [1, 1, 2, 1]

            # Move 1 sample, 1 height, 2 width, and 1 depth at a time
            strides = [1, 1, 2, 1]

            # Perform the max pooling
            pool = tf.nn.max_pool(output_2_reshaped, ksize, strides, padding='SAME')

            # Remove the "height" dimension again
            output_5 = tf.squeeze(pool, 1)

            # -------------------------------------------- LSTM --------------------------------------------
            lstm_units = 4

            LSTM_weights = [
                [0.047461336, -0.2525125, 0.133023, 0.11263981, 0.34583206, -0.61531019, -0.8803405, 0.99488366,
                 0.85897743, -0.36209199, -0.63935106, -0.05743177, 0.00316062, -0.61984265, -0.65267929, 0.25477407],
                [0.057721273, 0.0218620435, -0.01197869, 0.052430139, -0.28346617, -0.00683066, -0.71810405, 0.09335204,
                 0.62675115, 0.90367544, 0.39301098, -0.6796605, 0.77573529, 0.72351356, 0.30118468, -0.16851472],
                [-0.857999441, 0.023618397, 0.02000072, 0.185856544, -0.8450011, -0.7853939, -0.00261568, -0.70895696,
                 -0.20551959, -0.2137256, 0.04685076, 0.78725204, 0.6767832, 0.93165379, 0.24688992, 0.54482694],
                [-0.15851247, 0.18707229, -0.03427629, -0.089051459, 0.54172115, 0.94506127, -0.1827192, 0.44763495,
                 0.69120797, 0.29112628, 0.85072496, -0.0153968, -0.39041728, 0.18252702, 0.15263933, 0.35599324],
                [-0.83214537, 0.11154398, -0.028220953, -0.065826703, -0.46254772, -0.04844626, 0.58888535, 0.71568758,
                 -0.39430272, 0.78568475, -0.17918778, -0.79950794, 0.77709696, -0.21645212, 0.2234143, -0.85459363],
                [0.08630577, -0.073923609, 0.12680474, -0.96924272, 0.72734454, 0.0777216, -0.60380081, -0.9895412,
                 0.37282269, 0.16629956, 0.92285417, 0.86485604, -0.13370907, -0.75214074, 0.72669859, -0.261183],
                [0.72360051, 0.82933379, 0.06519956, 0.25991941, 0.11860591, -0.99293746, -0.08927943, -0.56968878,
                 -0.33370412, 0.09363034, 0.13263364, -0.72481075, 0.88884886, 0.41754448, 0.5463333, -0.80689945],
                [0.75863926, -0.16137513, 0.21030268, 0.05861826, 0.16492918, -0.12813282, -0.8740667, -0.0847981,
                 -0.52497674, -0.29709172, -0.3040518, 0.31963997, 0.24175961, -0.91323495, -0.61515323, 0.32519525]]

            LSTM_bias = [0.77461336, -0.28620435, 0.75000072, -0.89051459, -0.46254772, -0.04844626, -0.60380081,
                         -0.56968878, -0.52497674, 0.09363034, 0.92285417, 0.86485604, -0.77709696, 0.21645212,
                         0.15263933, 0.54482694]

            # Create lstm layer
            lstm = tf.contrib.rnn.LSTMCell(lstm_units,
                                           forget_bias=0.)

            # Put it into Multi RNN Cell
            lstm = tf.contrib.rnn.MultiRNNCell([lstm] * 1)

            # Let dynamic rnn setup the control flow (making while loops and stuff)
            output_lstm, _ = tf.nn.dynamic_rnn(lstm, output_5, dtype=tf.float32)

            # Use only the last hidden state
            output_6 = tf.slice(output_lstm, [0, 1, 0], [1, 1, 4])

            # Construct operation for assigning mock weights
            kernel = next(i for i in tf.global_variables() if i.shape == (8, 16))
            assign_kernel = kernel.assign(LSTM_weights)

            # Construct operation for assigning mock bias
            bias = next(i for i in tf.global_variables() if i.shape == (16,))
            assign_bias = bias.assign(LSTM_bias)

            # -------------------------------------------- FC 2--------------------------------------------
            weights_7 = tf.constant([[0.86485604, 0.76485604],
                                     [0.75214074, 0.65214074],
                                     [-0.13370907, -0.23370907],
                                     [0.72669859, 0.62669859]],
                                    dtype=tf.float32)

            bias_7 = tf.constant([0.85856544, 0.75856544], dtype=tf.float32)

            # Remove the "height" dimension to fit the required input shape of the matmul
            output_6_reshaped = tf.squeeze(output_6, 1)

            # Perform the matmul
            output_7 = tf.matmul(output_6_reshaped, weights_7) + bias_7

            # -------------------------------------------- Softmax -------------------------------------------

            output_final = tf.nn.softmax(output_7)

            # -------------------------------------------- LRP -------------------------------------------

            # Get the explanation from the LRP framework.
            R = lrp.lrp(inp, output_final)

            # Run the computations
            with tf.Session() as s:
                # Initialize variables
                s.run(tf.global_variables_initializer())

                # Assign mock bias
                s.run([assign_kernel, assign_bias])

                # # Calculate relevance
                relevances = s.run(R)

                # Expected result calculated in
                # https://docs.google.com/spreadsheets/d/1_bmSEBSWVOkpdlZYEUckgrnUtxhEfnR84LZy1cU5fIw/edit?usp=sharing
                expected_result = np.array([[0, 0.00009744890346, 0.00005380548547, 0.00006157330073, 0.0002224629534,
                                             0.000001148301027, 0, 0, 0.00009112270278, 0.0005228184956],
                                            [0.0003926704111, 0.000690260643, 0.00002775464123, 0.00002765821888,
                                             0.0001484110742, 0.0001955332403, 0.000003241945558, 0.0008382231393, 0,
                                             0],
                                            [0.0001084081589, 0.0003690088929, 0.0001606364539, 0, 0.0001548509328,
                                             0.00001354378626, 0, 0, 0.00002533766043, 0.000005178358804],
                                            [0.0002870030767, 0.0003006917612, 0.0005409557879, 0.001673765604,
                                             0.0005882002186, 0.0009939816563, 0.001152595479, 0.002409395133,
                                             0.00007871882761, 0.0001197214162],
                                            [0.003720715575, 0, 0.0008488224481, 0.001008967078, 0.007691724936,
                                             0.007605366069, 0.001017835398, 0.00411259372, 0.0005971349689,
                                             0.000256612695],
                                            [0, 0.0004449329458, 0, 0.01210797823, 0.00004847106631, 0.003506019562,
                                             0.001415993078, 0, 0.002416949139, 0.00006304742861],
                                            [0.005359096108, 0.005972816778, 0.002764248481, 0.007948120325,
                                             0.0003087310192, 0.01114928751, 0.0001196646517, 0.003871852483,
                                             0.0004964681915, 0.0003417654898],
                                            [0.02020125409, 0.0009174208261, 0.000388005489, 0.01936216584, 0,
                                             0.0243733322, 0.002747249697, 0.05362725729, 0.01680197517,
                                             0.02779999447]])

                # Check for shape and actual result
                self.assertEqual(inp.shape, R.shape)
                self.assertEqual(expected_result.shape, relevances.shape,
                                 "Shapes of expected relevance and relevance should be equal")
                self.assertTrue(np.allclose(relevances, expected_result, rtol=1e-03, atol=1e-03),
                                "The relevances do not match")
